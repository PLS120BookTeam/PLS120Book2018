<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title></title>
  <meta name="description" content="">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  


<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Applied Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path=""><a href="#ch.linreg"><i class="fa fa-check"></i><b>1</b> Linear Regression</a><ul>
<li class="chapter" data-level="1.1" data-path=""><a href="#learning-objectives-for-chapter"><i class="fa fa-check"></i><b>1.1</b> Learning Objectives for Chapter</a></li>
<li class="chapter" data-level="1.2" data-path=""><a href="#introduction-to-simple-linear-regression"><i class="fa fa-check"></i><b>1.2</b> Introduction to Simple Linear Regression</a></li>
<li class="chapter" data-level="1.3" data-path=""><a href="#simple-linear-regression-visualized"><i class="fa fa-check"></i><b>1.3</b> Simple Linear Regression Visualized</a></li>
<li class="chapter" data-level="1.4" data-path=""><a href="#fitting-the-least-squares-line"><i class="fa fa-check"></i><b>1.4</b> Fitting the least squares line</a></li>
<li class="chapter" data-level="1.5" data-path=""><a href="#analysis-of-variance-of-regression"><i class="fa fa-check"></i><b>1.5</b> Analysis of variance of regression</a></li>
<li class="chapter" data-level="1.6" data-path=""><a href="#confidence-interval-for-hatbeta_1"><i class="fa fa-check"></i><b>1.6</b> Confidence Interval for <span class="math inline">\(\hat{\beta_1}\)</span></a></li>
<li class="chapter" data-level="1.7" data-path=""><a href="#confidence-interval-for-hatbeta_0"><i class="fa fa-check"></i><b>1.7</b> Confidence Interval for <span class="math inline">\(\hat{\beta}_0\)</span></a></li>
<li class="chapter" data-level="1.8" data-path=""><a href="#confidence-intervals-for-haty_i"><i class="fa fa-check"></i><b>1.8</b> Confidence Intervals for <span class="math inline">\(\hat{Y}_i\)</span></a></li>
<li class="chapter" data-level="1.9" data-path=""><a href="#coefficient-of-determination-r2"><i class="fa fa-check"></i><b>1.9</b> Coefficient of Determination, <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="1.10" data-path=""><a href="#correlation-coefficient-r"><i class="fa fa-check"></i><b>1.10</b> Correlation Coefficient, r</a></li>
<li class="chapter" data-level="1.11" data-path=""><a href="#fitness-of-the-regression-model"><i class="fa fa-check"></i><b>1.11</b> Fitness of the regression model</a></li>
<li class="chapter" data-level="1.12" data-path=""><a href="#homework"><i class="fa fa-check"></i><b>1.12</b> Homework</a></li>
<li class="chapter" data-level="1.13" data-path=""><a href="#LabCh13PLS"><i class="fa fa-check"></i><b>1.13</b> Laboratory Exercises</a><ul>
<li class="chapter" data-level="1.13.1" data-path=""><a href="#plant-sciences"><i class="fa fa-check"></i><b>1.13.1</b> Plant Sciences</a></li>
<li class="chapter" data-level="1.13.2" data-path=""><a href="#part-1.-plot-of-data-and-estimation-of-parameters.-25-points"><i class="fa fa-check"></i><b>1.13.2</b> Part 1. Plot of data and estimation of parameters. [25 points]</a></li>
<li class="chapter" data-level="1.13.3" data-path=""><a href="#part-2.-test-of-null-hypothesis-and-r-square.-30-points"><i class="fa fa-check"></i><b>1.13.3</b> Part 2. Test of null hypothesis and R-square. [30 points]</a></li>
<li class="chapter" data-level="1.13.4" data-path=""><a href="#part-3.-make-a-95-confidence-interval-for-the-rgr-or-slope.-25-points"><i class="fa fa-check"></i><b>1.13.4</b> Part 3. Make a 95% confidence interval for the RGR or slope. [25 points]</a></li>
<li class="chapter" data-level="1.13.5" data-path=""><a href="#part-4.-make-a-95-confidence-interval-for-mean-plant-size-at-a-given-age.-20-points"><i class="fa fa-check"></i><b>1.13.5</b> Part 4. Make a 95% confidence interval for mean plant size at a given age. [20 points]</a></li>
<li class="chapter" data-level="1.13.6" data-path=""><a href="#animal-sciences"><i class="fa fa-check"></i><b>1.13.6</b> Animal Sciences</a></li>
<li class="chapter" data-level="1.13.7" data-path=""><a href="#part-1.-plot-of-data-and-estimation-of-parameters.-25-points-1"><i class="fa fa-check"></i><b>1.13.7</b> Part 1. Plot of data and estimation of parameters. [25 points]</a></li>
<li class="chapter" data-level="1.13.8" data-path=""><a href="#part-2.-test-of-null-hypothesis-and-r-square.-30-points-1"><i class="fa fa-check"></i><b>1.13.8</b> Part 2. Test of null hypothesis and R-square. [30 points]</a></li>
<li class="chapter" data-level="1.13.9" data-path=""><a href="#part-3.-make-a-95-confidence-interval-for-the-estimation-of-the-slope.-25-points"><i class="fa fa-check"></i><b>1.13.9</b> Part 3. Make a 95% confidence interval for the estimation of the slope. [25 points]</a></li>
<li class="chapter" data-level="1.13.10" data-path=""><a href="#part-4.-make-a-95-confidence-interval-for-mean-heart-weight-at-a-given-body-weight.-20-points"><i class="fa fa-check"></i><b>1.13.10</b> Part 4. Make a 95% confidence interval for mean heart weight at a given body weight. [20 points]</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<!--bookdown:title:end-->
<!--bookdown:title:start-->
<div id="ch.linreg" class="section level1">
<h1><span class="header-section-number">Chapter 1</span> Linear Regression</h1>
<div id="learning-objectives-for-chapter" class="section level2">
<h2><span class="header-section-number">1.1</span> Learning Objectives for Chapter</h2>
<ol style="list-style-type: decimal">
<li>For a simple linear regression, state the null and alternative hypotheses.</li>
<li>Describe how variance is partitioned in a linear regression.<br />
</li>
<li>Calculate the slope and intercept for a regression model and their CIs, and interpret what the values mean in real, biological terms.<br />
</li>
<li>Calculate the coefficient of determination and state what the value means.</li>
<li>Describe how ANOVA and regression analyses are related in terms of variance partitioning and the F test.<br />
</li>
<li>List examples of uses for simple linear regression (SLR).</li>
<li>Write the model and assumptions for simple linear regression.</li>
<li>Make confidence intervals for the SLR parameters.</li>
<li>List strategies to increase the precision of slope estimates.</li>
<li>Determine is a problem requires an estimate for the expected value or for a single observation.</li>
<li>Calculate and interpret an ANOVA table for SLR.</li>
<li>Discuss the difference between estimating parameters and making predictions in SLR.</li>
</ol>
</div>
<div id="introduction-to-simple-linear-regression" class="section level2">
<h2><span class="header-section-number">1.2</span> Introduction to Simple Linear Regression</h2>
<p>In simple linear regression, we are interested in the relationship between two continuous variables. More specifically, we are interested in how one variable (Y, the dependent variable) changes with each unit change in another variable (X, the independent variable). First, we construct a linear mathematical model that describes the change in Y with each unit change in X. This linear model takes the following form: <span class="math display">\[\hat{Y}_i = \beta_0 + \beta_1X_i + \epsilon_i \\ \epsilon_{i} \sim N(0, \ \sigma^2)\]</span> where <span class="math inline">\(\hat{Y}_i\)</span> is the predicted value of variable Y for a particular (<em>i</em>) value of variable X. The variable <span class="math inline">\(\epsilon_i\)</span> represents the random <em>observed</em> deviation from the model’s theoretical, predicted value of Y, and it is normally distributed with mean 0 and variance <span class="math inline">\(\sigma^2\)</span>. Each random deviation from <span class="math inline">\(\hat{Y}\)</span> (<span class="math inline">\(\epsilon_i\)</span>) is called a <strong>residual</strong>.</p>
<p>We use observed values of X and Y to estimate the parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. These are called the regression coefficients, and they have their own special interpretations. The <span class="math inline">\(\beta_0\)</span> coefficient is also called the <strong>intercept</strong>, and if you examine the regression model closely, you might realize that <span class="math inline">\(\beta_0\)</span> is simply <span class="math inline">\(\hat{Y}_i\)</span> when <span class="math inline">\(X_i\)</span> is equal to 0. The <span class="math inline">\(\beta_1\)</span> coefficient is also referred to as the <strong>slope</strong>, and it is a rate parameter that represents the change in Y for each unit change in X. We use a method called <strong>ordinary least squares</strong> to estimate these values. Once the regression coefficients are estimated, we can use a statistical test to determine if the relationship between X and Y is significant. If it is, we can then use our model to estimate or predict values of Y for a particular value of X.</p>
</div>
<div id="simple-linear-regression-visualized" class="section level2">
<h2><span class="header-section-number">1.3</span> Simple Linear Regression Visualized</h2>
As the independent and dependent variables in linear regression are both continuous, the relationship between X and Y can best be visualized with a scatterplot, as shown in Fig. <a href="#fig:slr">1.1</a>
<div class="figure"><span id="fig:slr"></span>
<img src="13.0_LinearRegression_files/figure-html/slr-1.png" alt="A scatterplot of simulated random data, with dots representing observations. The line represents the linear regression model used to generate the data, in combination with residuals drawn from a random normal distribution. An example residual is show in red." width="672" />
<p class="caption">
Figure 1.1: A scatterplot of simulated random data, with dots representing observations. The line represents the linear regression model used to generate the data, in combination with residuals drawn from a random normal distribution. An example residual is show in red.
</p>
</div>
<p>As mentioned above, we use ordinary least squares to estimate regression coefficients that represent a line of “best fit”. This method finds the <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> that minimizes the squared distances of observed values of Y from values of Y that are predicted from the regression equation, <span class="math inline">\(\hat{Y}\)</span>. Figure <a href="#fig:leastsquaresline">1.2</a> illustrates a least squares line versus alternative lines we could use to explain the relationship between X and Y.</p>
<div class="figure"><span id="fig:leastsquaresline"></span>
<img src="13.0_LinearRegression_files/figure-html/leastsquaresline-1.png" alt="The solid line is the line of 'best fit' that minimizes the sum of the squared deviations between the line and the observations, which are represented by the points. The dashed lines represent lines with the same $\overline{Y}$ but with different regression coefficients ($\beta_0$ and $\beta_1$). These lines do not minimize the overall error between $\hat{Y}_i$ and the observed Y$_i$" width="672" />
<p class="caption">
Figure 1.2: The solid line is the line of ‘best fit’ that minimizes the sum of the squared deviations between the line and the observations, which are represented by the points. The dashed lines represent lines with the same <span class="math inline">\(\overline{Y}\)</span> but with different regression coefficients (<span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>). These lines do not minimize the overall error between <span class="math inline">\(\hat{Y}_i\)</span> and the observed Y<span class="math inline">\(_i\)</span>
</p>
</div>
<p>We can use the following equations to estimate the regression coefficients, with <em>k</em> observations:</p>
<span class="math display" id="eq:b1">\[\begin{equation}
\beta_1 = \frac{\sum_{i=1}^k (X_i-\overline{X})(Y_i-\overline{Y})}{\sum_{i=1}^k (X_i-\overline{X})^2} 
\tag{1.1}
\end{equation}\]</span>
<span class="math display" id="eq:b0">\[\begin{equation}
\beta_0 = \overline{Y} - \beta_1\overline{X}
\tag{1.2}
\end{equation}\]</span>
</div>
<div id="fitting-the-least-squares-line" class="section level2">
<h2><span class="header-section-number">1.4</span> Fitting the least squares line</h2>
<p>An example of a linear relationship between an independent continuous variable (X) and a dependent continuous variable (Y) would be the relationship between annual rainfall and rice crop yield (kg/ha) in India. Figure <a href="#fig:rice">1.3</a> shows observed rice crop yields in India from 1990–2000 against annual rainfall levels.</p>
<div class="figure"><span id="fig:rice"></span>
<img src="13.0_LinearRegression_files/figure-html/rice-1.png" alt="Observed rice crop yields in India from 1990–2000 against annual rainfall levels" width="672" />
<p class="caption">
Figure 1.3: Observed rice crop yields in India from 1990–2000 against annual rainfall levels
</p>
</div>
<p>In table <a href="#tab:ricetab">1.1</a>, we show the values and calculations necessary to find the regression coefficients for the least squares regression line.</p>
<table>
<caption>
<span id="tab:ricetab">Table 1.1: </span>Table shows the rice crop yields in India (Y) from 1990–2000, along with annual rainfall (X), the difference between each X<span class="math inline">\(_i\)</span> and , the difference between each Y<span class="math inline">\(_i\)</span> and , the product of the X and Y deviations from their respective means (<span class="math inline">\(\Delta X\)</span><span class="math inline">\(\Delta Y\)</span>), and the squared deviations of X<span class="math inline">\(_i\)</span> from 
</caption>
<thead>
<tr>
<th style="text-align:right;">
<span class="math inline">\(X_i\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(Y_i\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\((X_i - \overline{X})\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\((Y_i - \overline{Y})\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\Delta X\)</span><span class="math inline">\(\Delta Y\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\Delta X^2\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
-10.5
</td>
<td style="text-align:right;">
-163.3
</td>
<td style="text-align:right;">
1714.7
</td>
<td style="text-align:right;">
110.2
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
-149.3
</td>
<td style="text-align:right;">
15
</td>
<td style="text-align:right;">
-2239.5
</td>
<td style="text-align:right;">
22290.5
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
147.3
</td>
<td style="text-align:right;">
-320
</td>
<td style="text-align:right;">
-47136
</td>
<td style="text-align:right;">
21697.3
</td>
</tr>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
-44.8
</td>
<td style="text-align:right;">
13
</td>
<td style="text-align:right;">
-582.4
</td>
<td style="text-align:right;">
2007
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
54.7
</td>
<td style="text-align:right;">
-80
</td>
<td style="text-align:right;">
-4376
</td>
<td style="text-align:right;">
2992.1
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
2.3
</td>
<td style="text-align:right;">
38
</td>
<td style="text-align:right;">
87.4
</td>
<td style="text-align:right;">
5.3
</td>
</tr>
<tr>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
49.5
</td>
<td style="text-align:right;">
67
</td>
<td style="text-align:right;">
3316.5
</td>
<td style="text-align:right;">
2450.2
</td>
</tr>
<tr>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
-55.7
</td>
<td style="text-align:right;">
138
</td>
<td style="text-align:right;">
-7686.6
</td>
<td style="text-align:right;">
3102.5
</td>
</tr>
<tr>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
-157.9
</td>
<td style="text-align:right;">
114
</td>
<td style="text-align:right;">
-18000.6
</td>
<td style="text-align:right;">
24932.4
</td>
</tr>
<tr>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
81.6
</td>
<td style="text-align:right;">
65.7
</td>
<td style="text-align:right;">
5361.1
</td>
<td style="text-align:right;">
6658.6
</td>
</tr>
<tr>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
82.3
</td>
<td style="text-align:right;">
113
</td>
<td style="text-align:right;">
9299.9
</td>
<td style="text-align:right;">
6773.3
</td>
</tr>
<tr>
<td style="text-align:right;font-weight: bold;">
</td>
<td style="text-align:right;font-weight: bold;">
</td>
<td style="text-align:right;font-weight: bold;">
</td>
<td style="text-align:right;font-weight: bold;">
Sums
</td>
<td style="text-align:right;font-weight: bold;">
-60241.5
</td>
<td style="text-align:right;font-weight: bold;">
93019.4
</td>
</tr>
</tbody>
</table>
<p>We can now use equation <a href="#eq:b1">(1.1)</a> to find the slope of the regression line that minimizes the error in the observations around the line:</p>
<span class="math display">\[\begin{equation}
\begin{aligned}
\sum_{i=1}^k (X_i-\overline{X})(Y_i-\overline{Y}) &amp; = -60241.5 \\
\sum_{i=1}^k (X_i-\overline{X})^2 &amp; = 93019.4 \\
\beta_1 &amp; = \frac{\sum (X_i-\overline{X})(Y_i-\overline{Y})}{\sum (X_i-\overline{X})^2} \\
\beta_1 &amp; = -60241.5/93019.4 \\
\beta_1 &amp; = -0.65
\end{aligned}
\end{equation}\]</span>
<p>And we can use <a href="#eq:b0">(1.2)</a> to find the intercept:</p>
<span class="math display">\[\begin{equation}
\begin{aligned}
\beta_0 &amp; = \overline{Y} - \beta_1\overline{X} \\
\overline{Y} &amp; = 2064 \\
\beta_1 &amp; = -0.65 \\
\overline{X} &amp; = 1130.7 \\
\beta_0 &amp; = 2064 - 0.65 \times 1130.7 \\
\beta_0 &amp; = 2799
\end{aligned}
\end{equation}\]</span>
<p>This gives us the regression equation:</p>
<span class="math display" id="eq:fittedrice">\[\begin{equation}
\hat{Y}_i = 2799 - 0.65 \times X_i + \epsilon_i
\tag{1.3}
\end{equation}\]</span>
<p>And we can plot this line on the original data scatterplot:</p>
<div class="figure"><span id="fig:riceline"></span>
<img src="13.0_LinearRegression_files/figure-html/riceline-1.png" alt="Observed rice crop yields in India from 1990–2000 against annual rainfall levels and the least squares regression line ($\beta_0 = 2799$kg/ha,  $\beta_1 = -0.65$)" width="672" />
<p class="caption">
Figure 1.4: Observed rice crop yields in India from 1990–2000 against annual rainfall levels and the least squares regression line (<span class="math inline">\(\beta_0 = 2799\)</span>kg/ha, <span class="math inline">\(\beta_1 = -0.65\)</span>)
</p>
</div>
<p>We have shown you above how to fit a simple linear regression line to explain the relationship between a predictor variable X and a dependent variable Y. We can state the direction of the relationship (negative) and report an effect size based on the estimated slope (for every increase in one unit of annual rainfall, there is a 0.65 decrease in projected rice crop yield). We can even state what we would expect the crop yield to be when there is no rain at all on a given year, which is simply the intercept (2,799 kg/ha).</p>
<p>But how do we know if the relationship that our regression line describes is significant? If you observe the scatter of the observations around the line in figure <a href="#fig:riceline">1.4</a>, you can see that the line does not perfectly predict crop yield given the annual rainfall. Those deviations are represented by the residual error symbol, <span class="math inline">\(\epsilon_i\)</span> in equation <a href="#eq:fittedrice">(1.3)</a>. The residual error is the random variation in crop yield (Y) that is not explained by our regression line. In other words, it is the variation in Y that is not explained by X. Is the residual error so large that our model of the relationship between X and Y is useless? In the next section, we will demonstrate how we test that the relationship between X and Y is significant, given the variation we see in Y.</p>
</div>
<div id="analysis-of-variance-of-regression" class="section level2">
<h2><span class="header-section-number">1.5</span> Analysis of variance of regression</h2>
<p>In Chapter 9, we introduced you to a method called the analysis of variance. The general idea of analysis of variance is that we can partition the total variance of a Y dependent variable into random variation and variation explained by an X independent variable in order to test if there is a significant effect of X on Y. In simple linear regression, we partition the total sum of squares of Y (SSY) into the sum of squares that can be explained by the regression line (SSR) and the sum of squares that is not explained by the regression line, also called the residual sum of squares (RSS). <span class="math inline">\(SSY = SSR + RSS\)</span>. In Figure <a href="#fig:anovaslr">1.5</a>, we show you each component of variation in Y. The total variation in Y has to do with the deviation of each <span class="math inline">\(Y_i\)</span> from <span class="math inline">\(\overline{Y}\)</span>. The regression line explains some of that variation in the deviations of each <span class="math inline">\(\hat{Y}_i\)</span> from <span class="math inline">\(\overline{Y}\)</span>. The remaining, residual variation is in the deviations of each observation <span class="math inline">\(Y_i\)</span> from<span class="math inline">\(\hat{Y}_i\)</span>.</p>
<div class="figure"><span id="fig:anovaslr"></span>
<img src="13.0_LinearRegression_files/figure-html/anovaslr-1.png" alt="Components of variation in Y. The total sum of squares represents the total variation in Y (SSY $= \sum (Y_i - \overline{Y})^2$. The sum of squares of the regression represents the variation that is explained by the regression line, and by extension, X (SSR $= \sum (\hat{Y}_i  - \overline{Y})^2$). The residual sum of squares represents the random variation in the scatter of Y observations around the regression line (RSS $= \sum (Y_i - \hat{Y}_i)^2$) " width="672" />
<p class="caption">
Figure 1.5: Components of variation in Y. The total sum of squares represents the total variation in Y (SSY <span class="math inline">\(= \sum (Y_i - \overline{Y})^2\)</span>. The sum of squares of the regression represents the variation that is explained by the regression line, and by extension, X (SSR <span class="math inline">\(= \sum (\hat{Y}_i - \overline{Y})^2\)</span>). The residual sum of squares represents the random variation in the scatter of Y observations around the regression line (RSS <span class="math inline">\(= \sum (Y_i - \hat{Y}_i)^2\)</span>)
</p>
</div>
<p>From the sum of squares (SS), we can obtain estimates of the <strong>variances</strong>, which are called the mean squares (MS). The MS is simply the SS divided by its corresponding degrees of freedom. In a simple linear regression, we are interested in comparing the variance explained by the regression (MSR) and the variance explained by the residuals (MSr). Specifically, we wish to obtain an F-ratio, which we calculate as the ratio of the variance explained by the regression to the residual variance <span class="math inline">\(\frac{MSR}{MSr}\)</span>.</p>
<p>We can summarize this information in an analysis of variance table.</p>
<table>
<caption>
<span id="tab:aovtable">Table 1.2: </span>Analysis of variance table.
</caption>
<thead>
<tr>
<th style="text-align:left;">
source
</th>
<th style="text-align:left;">
df
</th>
<th style="text-align:left;">
SS
</th>
<th style="text-align:left;">
MS
</th>
<th style="text-align:left;">
F-ratio
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Total
</td>
<td style="text-align:left;">
k-1
</td>
<td style="text-align:left;">
<span class="math inline">\(\sum (Y_i - \overline{Y})^2\)</span>
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
Regression
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
<span class="math inline">\(\sum (\hat{Y}_i - \overline{Y})^2\)</span>
</td>
<td style="text-align:left;">
MSR
</td>
<td style="text-align:left;">
F
</td>
</tr>
<tr>
<td style="text-align:left;">
Residual
</td>
<td style="text-align:left;">
k-2
</td>
<td style="text-align:left;">
<span class="math inline">\(\sum (Y_i - \hat{Y}_i)^2\)</span>
</td>
<td style="text-align:left;">
MSr
</td>
<td style="text-align:left;">
</td>
</tr>
</tbody>
</table>
<p>We can use the F-ratio to determine the significance of the regression. Specifically, the null hypothesis that we are interested in testing is that the regression coefficient (<span class="math inline">\(\beta_1\)</span>) = 0. If <span class="math inline">\(\beta_1\)</span> is equal to 0, then for each unit change in X, Y does not change, and there is no relationship between X and Y.</p>
<p>We will now demonstrate how to do an analysis of variance on the dataset from the previous section on rice crop yields in India.</p>
<p>First, we must calculate the different sources of variation in Y:</p>
<table>
<caption>
<span id="tab:SS">Table 1.3: </span>Table shows the rice crop yields in India (Y) from 1990–2000, along with annual rainfall (X), predicted crop yields based on regression equation <a href="#eq:fittedrice">(1.3)</a> (<span class="math inline">\(\hat{Y}_i\)</span>), the squared deviations in Y<span class="math inline">\(_i\)</span> from <span class="math inline">\(\overline{Y}\)</span>, the squared deviations between Y<span class="math inline">\(_i\)</span> and <span class="math inline">\(\hat{Y}_i\)</span>, and the squared deviations between <span class="math inline">\(\overline{Y}\)</span> and <span class="math inline">\(\hat{Y}\)</span>. The final row shows the sums of the squared deviations for the corresponding columns, representing the total sum of squares (SSY), sum of squares due to regression (SSR), and residual sum of squares (RSS)
</caption>
<thead>
<tr>
<th style="text-align:right;">
X<span class="math inline">\(_i\)</span>
</th>
<th style="text-align:right;">
Y<span class="math inline">\(_i\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\hat{Y}_i\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\((Y_i - \overline{Y})^2\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\((\hat{Y}_i - \overline{Y})^2\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\((Y_i - \hat{Y}_i)^2\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1120.2
</td>
<td style="text-align:right;">
1900.7
</td>
<td style="text-align:right;">
2070.87
</td>
<td style="text-align:right;">
26678.8
</td>
<td style="text-align:right;">
46.7
</td>
<td style="text-align:right;">
28958
</td>
</tr>
<tr>
<td style="text-align:right;">
981.4
</td>
<td style="text-align:right;">
2079
</td>
<td style="text-align:right;">
2161.09
</td>
<td style="text-align:right;">
223.9
</td>
<td style="text-align:right;">
9419.4
</td>
<td style="text-align:right;">
6739
</td>
</tr>
<tr>
<td style="text-align:right;">
1278
</td>
<td style="text-align:right;">
1744
</td>
<td style="text-align:right;">
1968.3
</td>
<td style="text-align:right;">
102423.3
</td>
<td style="text-align:right;">
9165.5
</td>
<td style="text-align:right;">
50310
</td>
</tr>
<tr>
<td style="text-align:right;">
1085.9
</td>
<td style="text-align:right;">
2077
</td>
<td style="text-align:right;">
2093.165
</td>
<td style="text-align:right;">
168.1
</td>
<td style="text-align:right;">
848.5
</td>
<td style="text-align:right;">
261
</td>
</tr>
<tr>
<td style="text-align:right;">
1185.4
</td>
<td style="text-align:right;">
1984
</td>
<td style="text-align:right;">
2028.49
</td>
<td style="text-align:right;">
6405.8
</td>
<td style="text-align:right;">
1263.5
</td>
<td style="text-align:right;">
1979
</td>
</tr>
<tr>
<td style="text-align:right;">
1133
</td>
<td style="text-align:right;">
2102
</td>
<td style="text-align:right;">
2062.55
</td>
<td style="text-align:right;">
1441.2
</td>
<td style="text-align:right;">
2.2
</td>
<td style="text-align:right;">
1556
</td>
</tr>
<tr>
<td style="text-align:right;">
1180.2
</td>
<td style="text-align:right;">
2131
</td>
<td style="text-align:right;">
2031.87
</td>
<td style="text-align:right;">
4484.1
</td>
<td style="text-align:right;">
1034.7
</td>
<td style="text-align:right;">
9827
</td>
</tr>
<tr>
<td style="text-align:right;">
1075
</td>
<td style="text-align:right;">
2202
</td>
<td style="text-align:right;">
2100.25
</td>
<td style="text-align:right;">
19034
</td>
<td style="text-align:right;">
1311.4
</td>
<td style="text-align:right;">
10353
</td>
</tr>
<tr>
<td style="text-align:right;">
972.8
</td>
<td style="text-align:right;">
2178
</td>
<td style="text-align:right;">
2166.68
</td>
<td style="text-align:right;">
12987.7
</td>
<td style="text-align:right;">
10535.7
</td>
<td style="text-align:right;">
128
</td>
</tr>
<tr>
<td style="text-align:right;">
1212.3
</td>
<td style="text-align:right;">
2129.7
</td>
<td style="text-align:right;">
2011.005
</td>
<td style="text-align:right;">
4311.7
</td>
<td style="text-align:right;">
2812.3
</td>
<td style="text-align:right;">
14089
</td>
</tr>
<tr>
<td style="text-align:right;">
1213
</td>
<td style="text-align:right;">
2177
</td>
<td style="text-align:right;">
2010.55
</td>
<td style="text-align:right;">
12760.8
</td>
<td style="text-align:right;">
2860.8
</td>
<td style="text-align:right;">
27706
</td>
</tr>
<tr>
<td style="text-align:right;font-weight: bold;">
</td>
<td style="text-align:right;font-weight: bold;">
</td>
<td style="text-align:right;font-weight: bold;">
</td>
<td style="text-align:right;font-weight: bold;">
SSY
</td>
<td style="text-align:right;font-weight: bold;">
SSR
</td>
<td style="text-align:right;font-weight: bold;">
RSS
</td>
</tr>
<tr>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
190919.4
</td>
<td style="text-align:right;">
39300.7
</td>
<td style="text-align:right;">
151906
</td>
</tr>
</tbody>
</table>
<p>No we can construct and analysis of variance table:</p>
<table>
<caption>
<span id="tab:riceaov">Table 1.4: </span>Analysis of variance table.
</caption>
<thead>
<tr>
<th style="text-align:left;">
source
</th>
<th style="text-align:left;">
df
</th>
<th style="text-align:left;">
SS
</th>
<th style="text-align:left;">
MS
</th>
<th style="text-align:left;">
F-ratio
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Total
</td>
<td style="text-align:left;">
10
</td>
<td style="text-align:left;">
190919.4
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
Regression
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
39300.9
</td>
<td style="text-align:left;">
39300.9
</td>
<td style="text-align:left;">
2.3
</td>
</tr>
<tr>
<td style="text-align:left;">
Residual
</td>
<td style="text-align:left;">
9
</td>
<td style="text-align:left;">
151906
</td>
<td style="text-align:left;">
16878.4
</td>
<td style="text-align:left;">
</td>
</tr>
</tbody>
</table>
<p>We now have an F-ratio which we can compare to a critical F-ratio given a significance probability threshold, from an F distribution with the appropriate degrees of freedom. The degrees of freedom that are relevant are from the numerator (<span class="math inline">\(df_1\)</span>) and denominator (<span class="math inline">\(df_2\)</span>) of the F-ratio. In the example above, <span class="math inline">\(df_1 = 1\)</span> and <span class="math inline">\(df_2 = 9\)</span>.</p>
<p>We can plot an F distribution with those degrees of freedom in R, using the following code:</p>
<p>Create a sequence of F values which will be plotted on the X axis of the F probability density distribution:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="fl">0.25</span>,<span class="dv">6</span>,<span class="fl">0.1</span>)</code></pre></div>
<p>Generate probability densities for the F values in x:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pd &lt;-<span class="st"> </span><span class="kw">df</span>(x, <span class="dt">df1 =</span> <span class="dv">1</span>, <span class="dt">df2 =</span> <span class="dv">9</span>, <span class="dt">ncp =</span> <span class="dv">0</span>)</code></pre></div>
<p>Plot the probability density of an F distribution with df1 = 1, df2 = 9:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(x, pd, 
     <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>,
     <span class="dt">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;&quot;</span>)</code></pre></div>
<p><img src="13.0_LinearRegression_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>We will use the typical threshold of probability for significance, where <span class="math inline">\(\alpha = 0.05\)</span> and obtain the critical F value corresponding to that probability:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">criticalF &lt;-<span class="st"> </span><span class="kw">qf</span>(<span class="dt">p =</span> <span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="fl">0.05</span>, 
                <span class="dt">df1 =</span> <span class="dv">1</span>, <span class="dt">df2 =</span> <span class="dv">9</span>)</code></pre></div>
<p>Note that we specified 0.05 instead of <span class="math inline">\(\alpha/2\)</span> = 0.025 because the F test for a linear regression is one-tailed.</p>
<p>Add a red dashed vertical line where that critical F value is located and a mark where our test F-ratio falls. Note the “col” argument changes the color of the line, and “lty” changes the line type to a dashed line“”</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(x, pd, 
     <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>,
     <span class="dt">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;&quot;</span>)

<span class="kw">abline</span>(<span class="dt">v =</span> criticalF, 
       <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, 
       <span class="dt">lty =</span> <span class="dv">3</span>)

<span class="kw">points</span>(<span class="fl">2.3</span>, <span class="kw">df</span>(<span class="fl">2.3</span>,<span class="dv">1</span>,<span class="dv">9</span>), <span class="dt">pch =</span> <span class="dv">4</span>)

<span class="kw">text</span>(<span class="fl">2.3</span>, <span class="fl">0.15</span>, <span class="st">&quot;F-ratio&quot;</span>)  </code></pre></div>
<p><img src="13.0_LinearRegression_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>As you can see, the F-ratio we calculated from our dataset on rice crop yields and annual rainfall was not significant. The test F-ratio was much smaller than the critical F value. We can calculate a p value for our test F ratio using the following code:</p>
<p>Note that we specify below the “lower.tail” is F so that it gives us the probability in the upper tail, to the right of where our F-ratio falls on the pdf.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pf</span>(<span class="fl">2.3</span>, 
   <span class="dt">df1 =</span> <span class="dv">1</span>, 
   <span class="dt">df2 =</span> <span class="dv">9</span>, 
   <span class="dt">lower.tail =</span> F)</code></pre></div>
<pre><code>## [1] 0.1636819</code></pre>
<p>This p value indicates that there is a 0.16 probability of seeing the results that we saw or more extreme results if the null hypothesis was true. This is larger than <span class="math inline">\(\alpha = 0.05\)</span>. We therefore fail to reject our null hypothesis that the slope of our regression line is equal to 0, and state that the relationship was not significant.</p>
<p>We can also use a built-in statistics function in R to do the simple linear regression all at once:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">X &lt;-<span class="st"> </span>rice[,<span class="dv">1</span>] <span class="co">#annual rainfall</span>
Y &lt;-<span class="st"> </span>rice[,<span class="dv">2</span>] <span class="co">#crop yield</span>

<span class="kw">summary</span>(<span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X))</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ X)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -224.61  -63.14   11.73  100.48  166.29 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 2796.2741   483.2150   5.787 0.000264 ***
## X             -0.6476     0.4260  -1.520 0.162746    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 129.9 on 9 degrees of freedom
## Multiple R-squared:  0.2043, Adjusted R-squared:  0.1159 
## F-statistic: 2.311 on 1 and 9 DF,  p-value: 0.1627</code></pre>
<p>Note that the F-ratio, the p value, and the regression coefficients are the same as what we calculated above. The estimate of (Intercept) is the same as <span class="math inline">\(\beta_0\)</span>, though it differs slightly from our hand calculations due to rounding error. The estimate of the “X” coefficient is the same as <span class="math inline">\(\beta_1\)</span>.</p>
<p>The summary output from the lm() function in R also reports a t value and a p value, [PR(&gt;|t|)], for each of the coefficients. We are generally only interested in the p value for the predictor variable coefficients, and can ignore the t test results for the <span class="math inline">\(\beta_0\)</span> coefficient. The p value that was reported for the “X” coefficient is the same as the p value reported for the F test. This is because there is only one X predictor in a simple linear regression, so all of the explained variance in Y is from the single X variable, annual rainfall. Doing a t test on the <span class="math inline">\(\beta_1\)</span> coefficient for X in this case will therefore yield the same results as doing an F test.</p>
</div>
<div id="confidence-interval-for-hatbeta_1" class="section level2">
<h2><span class="header-section-number">1.6</span> Confidence Interval for <span class="math inline">\(\hat{\beta_1}\)</span></h2>
<p>We can construct confidence intervals for an estimate of <span class="math inline">\(\beta_1\)</span> using the calculated MSr as an estimate of the population variance. The standard error of the regression coefficient is:</p>
<span class="math display" id="eq:sb">\[\begin{equation}
s_b = \sqrt{\frac{MSr}{\sum (X_i - \overline{X})^2}}
\tag{1.4}
\end{equation}\]</span>
<p>Note the two components of the standard error in our estimate of <span class="math inline">\(\beta_1\)</span>: the MSr and the SSX. If we want to increase our certainty in the slope of the regression, then we can either decrease our MSr by increasing sample size or increase the variation in X by increasing the range of X values for which we have measurements of Y.</p>
<p>In addition to <span class="math inline">\(s_b\)</span>, we need a t value to construct as confidence interval around the esimate of <span class="math inline">\(\beta_1\)</span>. We use the degrees of freedom from the residual variance (k-2) and calculate a t value under the null hypothesis that the true population <span class="math inline">\(\beta_1\)</span> = 0. We can use the following formula to obtain the t value:</p>
<span class="math display" id="eq:tsb">\[\begin{equation}
t = \frac{\hat{\beta}_1 - \beta_1}{s_b}, 
df = k-2
\tag{1.5}
\end{equation}\]</span>
<p>The confidence interval can then be calculated as:</p>
<span class="math display" id="eq:CIb">\[\begin{equation}
\hat{\beta}_1 \pm t_{\alpha, k-2} \times s_b
\tag{1.6}
\end{equation}\]</span>
</div>
<div id="confidence-interval-for-hatbeta_0" class="section level2">
<h2><span class="header-section-number">1.7</span> Confidence Interval for <span class="math inline">\(\hat{\beta}_0\)</span></h2>
<p>We can also construct confidence intervals for an estimate of <span class="math inline">\(\beta_0\)</span> The standard error of the intercept is:</p>
<span class="math display" id="eq:sa">\[\begin{equation}
s_a = \sqrt{MSr\Bigg(\frac{1}{k} + \frac{\overline{x}^2}{\sum (X_i - \overline{X})^2}\Bigg)}
\tag{1.7}
\end{equation}\]</span>
<p>where k is the number of observations.</p>
<p>Just as in the calculation of a t value for the <span class="math inline">\(\beta_1\)</span> confidence intervals, we use the degrees of freedom from the residual variance (k-2) to obtain a t value under the null hypothesis that the true population intercept = 0. We can use the following formula to obtain the t value:</p>
<span class="math display" id="eq:tsb">\[\begin{equation}
t = \frac{\hat{\beta}_0 - \beta_0}{s_a},
df = k-2
\tag{1.5}
\end{equation}\]</span>
<p>The confidence interval can then be calculated as:</p>
<span class="math display" id="eq:CIb">\[\begin{equation}
\hat{\beta}_0 \pm t_{\alpha, k-2} \times s_a
\tag{1.6}
\end{equation}\]</span>
</div>
<div id="confidence-intervals-for-haty_i" class="section level2">
<h2><span class="header-section-number">1.8</span> Confidence Intervals for <span class="math inline">\(\hat{Y}_i\)</span></h2>
<p>The predicted value for Y at a given X<span class="math inline">\(_i\)</span> can be considered in two different ways. For X<span class="math inline">\(_i\)</span>, <span class="math inline">\(\hat{Y}_i\)</span> can represent the expected mean of Y, or it can represent the expected value of an indivdual Y. The predicted value will be the same for either case, but our certainty in the estimate will be different.<br />
Predicting for a single Y means incorporating random variation at observation i. Recall that variation is represented by <span class="math inline">\(\epsilon_i\)</span> in the regression formula, and that visually it is represented by the scatter of points around the regression line on a scatterplot. We must account for the individual variance in the Y observations at X<span class="math inline">\(_i\)</span> for an individual <span class="math inline">\(\hat{Y_i}\)</span>, so the confidence intervals for an individual <span class="math inline">\(\hat{Y_i}\)</span> will be larger.</p>
<p>Just as in the calculations for the standard error of <span class="math inline">\(\hat{\beta}_1\)</span> and <span class="math inline">\(\hat{\beta}_0\)</span>, we will use the degrees of freedom of the MSr (k-2) in our calculations.</p>
<p>The standard error of <span class="math inline">\(\hat{Y_i}\)</span> for an expected mean of Y is:</p>
<span class="math display" id="eq:symean">\[\begin{equation}
s_{\hat{\overline{Y}}} = \sqrt{MSr\Bigg(\frac{1}{k} + \frac{(x_i - \overline{x})^2}{\sum (X_i - \overline{X})^2}\Bigg)}
\tag{1.8}
\end{equation}\]</span>
<p>while the standard error of <span class="math inline">\(\hat{Y_i}\)</span> for an individual Y is</p>
<span class="math display" id="eq:sy">\[\begin{equation}
s_{\hat{Y}} = \sqrt{MSr\Bigg(1 + \frac{1}{k} + \frac{(x_i - \overline{x})^2}{\sum (X_i - \overline{X})^2}\Bigg)}
\tag{1.9}
\end{equation}\]</span>
<p>Note the addition of 1 in the calculation of <span class="math inline">\(s_{\hat{Y}}\)</span> to account for the additional residual variance when predicting an individual Y<span class="math inline">\(_i\)</span>.</p>
<p>The confidence interval for <span class="math inline">\(\hat{\overline{Y}}\)</span> is</p>
<span class="math display" id="eq:CIsymean">\[\begin{equation}
\hat{\overline{Y}} \pm t_{\alpha, k-2} \times s_{\hat{\overline{Y}}}
\tag{1.10}
\end{equation}\]</span>
<p>and the confidence interval for <span class="math inline">\(\hat{Y}\)</span> is</p>
<span class="math display" id="eq:CIsy">\[\begin{equation}
\hat{Y} \pm t_{\alpha, k-2} \times s_{\hat{Y}}
\tag{1.11}
\end{equation}\]</span>
<p>.</p>
</div>
<div id="coefficient-of-determination-r2" class="section level2">
<h2><span class="header-section-number">1.9</span> Coefficient of Determination, <span class="math inline">\(R^2\)</span></h2>
<p>The coefficient of determination (<span class="math inline">\(R^2\)</span>) is the proportion of variance that is explained by the regression line. If a large proportion of the variation in Y is not due to the regression line, then even if you obtain significant results, X is not a very good predictor of Y. The formula for <span class="math inline">\(R^2\)</span> is</p>
<span class="math display" id="eq:r2">\[\begin{equation}
R^2 = \frac{SSR}{SSY}
\tag{1.12}
\end{equation}\]</span>
<p>For a simple linear regression with only one X predictor, <span class="math inline">\(R^2\)</span> is simply referred to as <span class="math inline">\(r^2\)</span>.</p>
<p>In some cases, we may choose to adjust the <span class="math inline">\(R^2\)</span> value (<span class="math inline">\(R^2_{adj}\)</span> for the degrees of freedom from the regression, which is only 1 in a simple linear regression.</p>
<span class="math display" id="eq:radj">\[\begin{equation}
R^2_{adj} = 1 - frac{MSr}{MSY}
\tag{1.13}
\end{equation}\]</span>
<p>Using the example of rice crop yields in India, we can calculate an <span class="math inline">\(r^2\)</span> of</p>
<span class="math display" id="eq:r2">\[\begin{equation}
\begin{aligned}
r^2 &amp; = \frac{39300.7}{190919.4} \\
&amp; = 0.21
\end{aligned}
\tag{1.12}
\end{equation}\]</span>
<p>We can multiply <span class="math inline">\(r^2\)</span> by 100 to state that 21% of the variation in annual rice crop yields was explained by annual rainfall.</p>
<p>We can also calculate <span class="math inline">\(R^2_adj\)</span> as</p>
<span class="math display" id="eq:r2">\[\begin{equation}
\begin{aligned}
r^2 &amp; = \frac{39300.7}{190919.4} \\
&amp; = 0.21
\end{aligned}
\tag{1.12}
\end{equation}\]</span>
<p>Let’s look again at the summary output from the lm() function in R:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">X &lt;-<span class="st"> </span>rice[,<span class="dv">1</span>] <span class="co">#annual rainfall</span>
Y &lt;-<span class="st"> </span>rice[,<span class="dv">2</span>] <span class="co">#crop yield</span>

<span class="kw">summary</span>(<span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X))</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ X)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -224.61  -63.14   11.73  100.48  166.29 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 2796.2741   483.2150   5.787 0.000264 ***
## X             -0.6476     0.4260  -1.520 0.162746    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 129.9 on 9 degrees of freedom
## Multiple R-squared:  0.2043, Adjusted R-squared:  0.1159 
## F-statistic: 2.311 on 1 and 9 DF,  p-value: 0.1627</code></pre>
<p>The lm() funtion reports that the “Multiple R-squared” = 0.2043, which is equivalent to our calculation of <span class="math inline">\(r^2\)</span>. The small difference between our calculations and what lm() reports is again due to rounding error.</p>
<p>Our results were not significant, but if they were, then 20.4% would indicate that a fairly large proportion of the variance in Y is not explained by X. This is a way of ascertaining fit of the regression model in the absence of replication at X<span class="math inline">\(_i\)</span>.</p>
</div>
<div id="correlation-coefficient-r" class="section level2">
<h2><span class="header-section-number">1.10</span> Correlation Coefficient, r</h2>
<p>The correlation coefficient, r, is a measure of the linear correlation between two variables. It does not imply a causal relationship between a fixed X variable and a Y dependent variable as regression does.</p>
<p>We calculate r using the following formula:</p>
<span class="math display" id="eq:r">\[\begin{equation}
r = \frac{\sum(X_i - \overline{X})(Y_i - \overline{Y})}{\sqrt{\sum (X_i - \overline{X})^2 \times \sum (Y_i - \overline{Y})^2}} 
\tag{1.14}
\end{equation}\]</span>
<p>The correlation coefficient ranges from -1 to 1. A perfect positive linear relationship between two variables would result in <span class="math inline">\(r = 1\)</span>, while a perfect negative relationship would results in <span class="math inline">\(r = -1\)</span>. No relationship is indicated by <span class="math inline">\(r = 0\)</span>.</p>
</div>
<div id="fitness-of-the-regression-model" class="section level2">
<h2><span class="header-section-number">1.11</span> Fitness of the regression model</h2>
<p>Another aspect of using linear regression to test the significance of a relationship between X and Y is whether or not the test is appropriate for the data. There are certain assumptions that must be met to ## Exercises and Solutions</p>
</div>
<div id="homework" class="section level2">
<h2><span class="header-section-number">1.12</span> Homework</h2>
</div>
<div id="LabCh13PLS" class="section level2">
<h2><span class="header-section-number">1.13</span> Laboratory Exercises</h2>
<p>Prepare an .Rmd document starting with the following text, where you substitute the corresponsing information for author name and date.</p>
<pre><code>---
title: &quot;Lab13 Simple Linear Regression&quot;
author: &quot;YourFirstName YourLastName&quot;
date: &quot;enter date here&quot;
output: html_document
---</code></pre>
<div id="plant-sciences" class="section level3">
<h3><span class="header-section-number">1.13.1</span> Plant Sciences</h3>
<p>In simple linear regression (SLR) we study the relationship between a <em>predictor, explanatory, or independent</em> variable (X, horizontal axis) and a <em>response or dependent</em> variable (Y, vertical axis). Different goals may be achieved with SLR. We may be interested in determining how much Y changes as X changes, or we may be interested in estimating the value of Y for values of X that were not observed. All goals require that we estimate parameters of a linear function for a straight line that best fits the data, which is represented by the linear model:</p>
<p><span class="math display">\[Y_{i} = \beta_{0} + \beta_{1}X_{i} + \epsilon_{i} \\ \epsilon_{i} \sim N(0, \ \sigma)\]</span> Best fit is achieved by minimizing the sum of squares of residuals or vertical distances between observations and the line of fit. The estimated parameters are the <em>slope</em> (<span class="math inline">\(\beta_{1}\)</span>) tangent of the angle between the line and the horizontal, and the <em>intercept</em> (<span class="math inline">\(\beta_{0}\)</span>), the height of the line when X = 0. Numerically, the <em>slope</em> is the rate of change that occurs between the Y (dependent) variable and the X (predictor) variable, or in other words, how the values of Y change with each unit change in the X variable. The <em>intercept</em> is the value the Y variable takes when the X variable is equal to 0.</p>
<p>In this exercise we use the clover.txt data set used in lab02. These data represent the mass of clover plants grown for different periods at three different temperatures. Temperatures are in the first column coded as 1 for 5-15 C, 2 for 10-20 C and 3 for 15-25 C. The second column contains the number of days of growth and the last column contains the log of the plant mass in grams (g). Column names are in the first row of the file, so we specify header = TRUE in the line of code to read the data. Data are placed in a data frame named “clover.</p>
<p>The relationship between plant size (Y, log transformed) and plant age (X, in days) is modeled as a straight line whose slope is the relative growth rate (RGR) of the plants. Relative growth rate is the amount of growth per unit time and per unit size of the plant. Relative growth rate is to plant size as interest rate is to a debt.</p>
<p>FOR THIS LAB WE IGNORE THE TEMPERATURE GROUPS. RGR means relative growth rate and is estimated by the slope of the regression.</p>
</div>
<div id="part-1.-plot-of-data-and-estimation-of-parameters.-25-points" class="section level3">
<h3><span class="header-section-number">1.13.2</span> Part 1. Plot of data and estimation of parameters. [25 points]</h3>
<p>1A) Make a scatterplot of plant size vs. age, ignoring the temperature groups. 1B) Obtain estimates for the slope and intercept and interpret the results in terms of plant growth.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">clover &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;./Datasets/Lab01clover.txt&quot;</span>, <span class="dt">header =</span> <span class="ot">TRUE</span>)
<span class="kw">str</span>(clover)</code></pre></div>
<pre><code>## &#39;data.frame&#39;:    45 obs. of  3 variables:
##  $ temp: int  1 1 1 1 1 1 1 1 1 1 ...
##  $ days: int  9 9 9 15 15 15 22 22 22 30 ...
##  $ lnwt: num  2.03 2.03 2 2.72 2.26 ...</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(lnwt <span class="op">~</span><span class="st"> </span>days, <span class="dt">data =</span> clover)</code></pre></div>
<p><img src="13.0_LinearRegression_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">slr1 &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="dt">formula =</span> lnwt <span class="op">~</span><span class="st"> </span>days, <span class="dt">data =</span> clover)
<span class="kw">summary</span>(slr1)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = lnwt ~ days, data = clover)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.15780 -0.39392 -0.06882  0.35453  0.86617 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 1.612187   0.196148   8.219 2.32e-10 ***
## days        0.089780   0.007991  11.235 2.24e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.5381 on 43 degrees of freedom
## Multiple R-squared:  0.7459, Adjusted R-squared:   0.74 
## F-statistic: 126.2 on 1 and 43 DF,  p-value: 2.24e-14</code></pre>
<p><strong>ANSWER THE FOLLOWING QUESTIONS:</strong></p>
<p>1B) What are the estimates for the slope and intercept? Interpret the results in terms of plant growth.</p>
</div>
<div id="part-2.-test-of-null-hypothesis-and-r-square.-30-points" class="section level3">
<h3><span class="header-section-number">1.13.3</span> Part 2. Test of null hypothesis and R-square. [30 points]</h3>
<p>Test the null hypothesis that RGR = 0 by performing an ANOVA for regression. Do the calculations “by hand” and then repeat using R functions.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">coef</span>(slr1) <span class="co"># Use help(coef) to understand what this function does</span></code></pre></div>
<pre><code>## (Intercept)        days 
##  1.61218702  0.08978046</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">intcpt &lt;-<span class="st"> </span><span class="kw">coef</span>(slr1)[<span class="dv">1</span>] <span class="co"># extract estimated intercept, or &quot;a&quot; as it is called in the SG textbook</span>

slope &lt;-<span class="st"> </span><span class="kw">coef</span>(slr1)[<span class="dv">2</span>] <span class="co"># Code needed here. Extract the estimated slope or b, just like we did with the intercept. </span>
  
  
lnwt.hat &lt;-<span class="st"> </span>intcpt <span class="op">+</span><span class="st"> </span>slope <span class="op">*</span><span class="st"> </span>clover<span class="op">$</span>days <span class="co"># creates a vector of estimated or predicted lnwt’s for each of the measured days using the regression model coefficients</span>


errors &lt;-<span class="st"> </span>clover<span class="op">$</span>lnwt <span class="op">-</span><span class="st"> </span>lnwt.hat <span class="co"># creates a vector of the differences between the sample lnwt measurements and the predicted weights from the model, i.e., these are the “residuals”</span>


(TotalSS &lt;-<span class="st"> </span><span class="kw">sum</span>((clover<span class="op">$</span>lnwt <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(clover<span class="op">$</span>lnwt)) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>)) <span class="co"># total sums of squares</span></code></pre></div>
<pre><code>## [1] 49.00412</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(SSErrors &lt;-<span class="st"> </span><span class="kw">sum</span>(errors <span class="op">^</span><span class="st"> </span><span class="dv">2</span>)) <span class="co"># Code needed here. Type in the vector of the residuals to calculate residual sum of squares</span></code></pre></div>
<pre><code>## [1] 12.45122</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(SSReg &lt;-<span class="st"> </span>TotalSS <span class="op">-</span><span class="st"> </span>SSErrors) <span class="co"># regression sums of squares</span></code></pre></div>
<pre><code>## [1] 36.5529</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n &lt;-<span class="st"> </span><span class="kw">length</span>(clover<span class="op">$</span>lnwt) <span class="co"># Code needed here. Complete the code with the column that contains the observations</span>

(dfe &lt;-<span class="st"> </span>n <span class="op">-</span><span class="st"> </span><span class="kw">length</span>(<span class="kw">coef</span>(slr1))) <span class="co"># df of the residuals</span></code></pre></div>
<pre><code>## [1] 43</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(dfTotal &lt;-<span class="st"> </span>n<span class="op">-</span><span class="dv">1</span>) <span class="co"># Complete the code to calculate the total df</span></code></pre></div>
<pre><code>## [1] 44</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(dfReg &lt;-<span class="st"> </span><span class="dv">1</span>) <span class="co"># Complete the code to calculate the df of the regression</span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(MSReg &lt;-<span class="st"> </span>SSReg <span class="op">/</span><span class="st"> </span>dfReg)</code></pre></div>
<pre><code>## [1] 36.5529</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(MSE &lt;-<span class="st"> </span>SSErrors <span class="op">/</span><span class="st"> </span>dfe) <span class="co"># Complete the code to calculate the MSE </span></code></pre></div>
<pre><code>## [1] 0.2895632</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(Fcalc &lt;-<span class="st"> </span>MSReg <span class="op">/</span><span class="st"> </span>MSE) <span class="co"># Calculated F statistic from the data </span></code></pre></div>
<pre><code>## [1] 126.2346</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(Fcritical &lt;-<span class="st"> </span><span class="kw">qf</span>(<span class="fl">0.95</span>, dfReg, dfe)) <span class="co"># Complete the function arguments with the degrees of freedom for the numerator and the denominator to get the F critical value.</span></code></pre></div>
<pre><code>## [1] 4.067047</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(slr1) <span class="co"># check your previous “by hand” calculation against the anova table.</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: lnwt
##           Df Sum Sq Mean Sq F value   Pr(&gt;F)    
## days       1 36.553  36.553  126.23 2.24e-14 ***
## Residuals 43 12.451   0.290                     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p><strong>ANSWER THE FOLLOWING QUESTIONS:</strong></p>
<p>2A) Do you reject the Ho: <span class="math inline">\(\beta_{1}\)</span> = 0? Why? What does this mean in terms of plant growth, in non-technical terms.</p>
<p>2B) What proportion of the variance of lnwt is explained by days, the predictor?</p>
</div>
<div id="part-3.-make-a-95-confidence-interval-for-the-rgr-or-slope.-25-points" class="section level3">
<h3><span class="header-section-number">1.13.4</span> Part 3. Make a 95% confidence interval for the RGR or slope. [25 points]</h3>
<p>First, do the calculations by hand, using the formulas from the textbook 179-180. Then use R functions.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Hint: check your answers with the anova table. It should be the same if your calculations are correct</span>


(tcritical.slr1 &lt;-<span class="st"> </span><span class="kw">qt</span>(<span class="fl">0.975</span>, dfe)) <span class="co"># Code needed here.  Enter the probability for a 2-tailed test with alpha = 0.05</span></code></pre></div>
<pre><code>## [1] 2.016692</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">SSX &lt;-<span class="st"> </span><span class="kw">sum</span>((clover<span class="op">$</span>days <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(clover<span class="op">$</span>days)) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>) <span class="co"># get sum of square of X</span>


(se.beta &lt;-<span class="st"> </span><span class="kw">sqrt</span>(SSErrors <span class="op">/</span><span class="st"> </span>(dfe <span class="op">*</span><span class="st"> </span>SSX))) <span class="co"># calculate the s.e. for beta</span></code></pre></div>
<pre><code>## [1] 0.007990844</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">beta.lo &lt;-<span class="st"> </span>slope <span class="op">-</span><span class="st"> </span>tcritical.slr1 <span class="op">*</span><span class="st"> </span>se.beta <span class="co"># lower CI boundary.  </span>


beta.hi &lt;-<span class="st"> </span>slope <span class="op">+</span><span class="st"> </span>tcritical.slr1 <span class="op">*</span><span class="st"> </span>se.beta <span class="co"># Code needed here. Complete the formula for the the upper CI boundary</span>


<span class="kw">c</span>(beta.lo, beta.hi) <span class="co"># Display the lower and upper CI extremes </span></code></pre></div>
<pre><code>##       days       days 
## 0.07366539 0.10589553</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(slr1)[<span class="dv">2</span>,] <span class="co"># easier way to get the CI using the function from R</span></code></pre></div>
<pre><code>##      2.5 %     97.5 % 
## 0.07366539 0.10589553</code></pre>
<p><strong>ANSWER THE FOLLOWING QUESTIONS:</strong></p>
<p>3A) What is the 95% confidence interval for the relative growth rate (RGR) of the plants?</p>
</div>
<div id="part-4.-make-a-95-confidence-interval-for-mean-plant-size-at-a-given-age.-20-points" class="section level3">
<h3><span class="header-section-number">1.13.5</span> Part 4. Make a 95% confidence interval for mean plant size at a given age. [20 points]</h3>
<p>Assume that you are interested in using the fitted regression model (the line) to estimate the mean mass of plants when they are 33 days old. Use the regression equation to make the estimate and then calculate the standard error of the estimate to make a confidence interval for it. Do detailed calculations first and then use R functions.</p>
<p>You will first calculate the predicted mean of Y for 33 days and its standard error and make a confidence interval for it. Then you will calculate the standard error for a predicted individual Y and compare it.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(lnwt.hat.<span class="dv">33</span> &lt;-<span class="st"> </span>intcpt <span class="op">+</span><span class="st"> </span>slope <span class="op">*</span><span class="st"> </span><span class="dv">33</span>) <span class="co"># In other words, Y = a + bX, where Y, or the predicted value, is the weight of plants at 33 days.  </span></code></pre></div>
<pre><code>## (Intercept) 
##    4.574942</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(mass.hat.<span class="dv">33</span> &lt;-<span class="st"> </span><span class="kw">exp</span>(lnwt.hat.<span class="dv">33</span>))  <span class="co"># Since the response variable is log transformed, here we need to do a back transformation.</span></code></pre></div>
<pre><code>## (Intercept) 
##    97.02243</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Calculate the standard error of the predicted mean of Y:</span>
se.lnwt.<span class="fl">33.</span>m &lt;-<span class="st"> </span><span class="kw">sqrt</span>(MSE <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">/</span><span class="st"> </span>n <span class="op">+</span><span class="st"> </span>(<span class="dv">33</span> <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(clover<span class="op">$</span>days)) <span class="op">^</span><span class="st"> </span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span>SSX)) <span class="co"># SG book pg. 192 and pg. d205 No. 6</span>


<span class="co"># Complete the code to calculate the upper and lower CI for the mass of plants at 33 days:</span>
(CI.lnwt.up &lt;-<span class="st"> </span>lnwt.hat.<span class="dv">33</span> <span class="op">+</span><span class="st"> </span>tcritical.slr1 <span class="op">*</span><span class="st"> </span>se.lnwt.<span class="fl">33.</span>m)</code></pre></div>
<pre><code>## (Intercept) 
##    4.810208</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(CI.lnwt.lo &lt;-<span class="st"> </span>lnwt.hat.<span class="dv">33</span> <span class="op">-</span><span class="st"> </span>tcritical.slr1 <span class="op">*</span><span class="st"> </span>se.lnwt.<span class="fl">33.</span>m)</code></pre></div>
<pre><code>## (Intercept) 
##    4.339677</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pred.data &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">lnwt =</span> <span class="ot">NA</span>, <span class="dt">group =</span> <span class="ot">NA</span>, <span class="dt">days =</span> <span class="dv">33</span>) <span class="co"># Here we create a data frame that we will use to “predict” or estimate lnwt in the next line.  Only the value for days is included since that is the X, independent, or predictor variable.</span>


(ci.r &lt;-<span class="st"> </span><span class="kw">predict</span>(slr1, <span class="dt">newdata =</span> pred.data, <span class="dt">interval =</span> <span class="st">&quot;confidence&quot;</span>)) <span class="co"># this line uses predict() to predict or estimate lnwt and its CI, as we did by hand above.</span></code></pre></div>
<pre><code>##        fit      lwr      upr
## 1 4.574942 4.339677 4.810208</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># The interval = “confidence” argument makes the function return a confidence interval for the estimated log of plant mass.</span>


<span class="kw">exp</span>(ci.r) <span class="co"># Complete the code to do a back transformation on the predicted values.</span></code></pre></div>
<pre><code>##        fit      lwr      upr
## 1 97.02243 76.68276 122.7571</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Calculate the standard error of the predicted individual Y:</span>
se.lnwt.<span class="fl">33.</span>ind &lt;-<span class="st"> </span><span class="kw">sqrt</span>(MSE <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span>n <span class="op">+</span><span class="st"> </span>(<span class="dv">33</span> <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(clover<span class="op">$</span>days)) <span class="op">^</span><span class="st"> </span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span>SSX)) <span class="co"># SG book pg. 205 No. 7</span></code></pre></div>
<p><strong>ANSWER THE FOLLOWING QUESTIONS:</strong></p>
<p>4a) Explain why we need to use exp().</p>
<p>4b) Report the upper and lower extremes of the CI for the predicted mean of Y</p>
<p>4c) Is the standard error estimate for a predicted mean of Y or a predicted individual Y based on the model bigger? Why?</p>
</div>
<div id="animal-sciences" class="section level3">
<h3><span class="header-section-number">1.13.6</span> Animal Sciences</h3>
<p>The same instructions for completion and submission of work used in previous labs applies here. Refer to previous labs for the details.</p>
<p>In simple linear regression (SLR) we study the relationship between a <em>predictor, explanatory or independent</em> variable (X, horizontal axis) and a <em>response or dependent</em> variable (Y, vertical axis). Different goals may be achieved with SLR. We may be interested in determining how much Y changes as X changes, or we may be interested in estimating the value of Y for values of X that were not observed. All goals require that we estimate parameters of a linear function for a straight line that best fits the data:</p>
<p><span class="math display">\[Y_{i} = \beta_{0} + \beta_{1}X_{i} + \epsilon_{i} \\ \epsilon_{i} \sim N(0, \ \sigma)\]</span></p>
<p>Best fit is achieved by minimizing the sum of squares of residuals or vertical distances between observations and the line of fit. The estimated parameters are the <em>slope</em> (<span class="math inline">\(\beta_{1}\)</span>) tangent of the angle between the line and the horizontal, and the <em>intercept</em> (<span class="math inline">\(\beta_{0}\)</span>), the height of the line when X = 0. Numerically, the <em>slope</em> is the rate of change that occurs between the Y (dependent) variable and the X (predictor) variable, or in other words, how the values of Y change with each unit change in the X variable. The <em>intercept</em> is the value the Y variable takes when the X variable is equal to 0.</p>
<p>In this exercise we will examine the relationship between body weight and heart weight in cats. This data was originally published in a short paper by R.A. Fisher in 1947 (see paper in CANVAS). R.A. Fisher was a British Statistician that developed the foundations for modern statistical theory.</p>
<p>Data for the exercise consists of body weight (Bwt) in kilograms and heart weight (Hwt) in grams of 144 cats. We will examine the relationship between heart weight and body weight in cats.</p>
</div>
<div id="part-1.-plot-of-data-and-estimation-of-parameters.-25-points-1" class="section level3">
<h3><span class="header-section-number">1.13.7</span> Part 1. Plot of data and estimation of parameters. [25 points]</h3>
<p>1A) Make a scatterplot of body weight vs. heart weight. 1B) Obtain estimates for the slope and intercept and interpret the results in terms of heart weight.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cats &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;./Datasets/Lab13Cats.csv&quot;</span>)
<span class="kw">str</span>(cats)</code></pre></div>
<pre><code>## &#39;data.frame&#39;:    144 obs. of  3 variables:
##  $ Sex   : Factor w/ 2 levels &quot;F&quot;,&quot;M&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ Bwt_Kg: num  2 2 2 2.1 2.1 2.1 2.1 2.1 2.1 2.1 ...
##  $ Hwt_g : num  7 7.4 9.5 7.2 7.3 7.6 8.1 8.2 8.3 8.5 ...</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(Bwt_Kg <span class="op">~</span><span class="st"> </span>Hwt_g, <span class="dt">data =</span> cats)</code></pre></div>
<p><img src="13.0_LinearRegression_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">slr1 &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="dt">formula =</span> Bwt_Kg <span class="op">~</span><span class="st"> </span>Hwt_g, <span class="dt">data =</span> cats)
<span class="kw">summary</span>(slr1)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Bwt_Kg ~ Hwt_g, data = cats)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.58283 -0.22140 -0.00879  0.20825  0.91717 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 1.019637   0.108428   9.404   &lt;2e-16 ***
## Hwt_g       0.160290   0.009944  16.119   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.2895 on 142 degrees of freedom
## Multiple R-squared:  0.6466, Adjusted R-squared:  0.6441 
## F-statistic: 259.8 on 1 and 142 DF,  p-value: &lt; 2.2e-16</code></pre>
<p><strong>ANSWER THE FOLLOWING QUESTIONS:</strong></p>
<p>1B) What are the estimates for the slope and intercept? Interpret the results in terms of heart weight and its relationship to body weight.</p>
</div>
<div id="part-2.-test-of-null-hypothesis-and-r-square.-30-points-1" class="section level3">
<h3><span class="header-section-number">1.13.8</span> Part 2. Test of null hypothesis and R-square. [30 points]</h3>
<p>Test the null hypothesis that there is no relationship between body weight and heart weight (i.e., <span class="math inline">\(\beta_{1}\)</span> = 0) by performing an ANOVA for regression. Do the calculations “by hand” and then repeat using R functions.</p>
<p>2A) Do you reject the Ho: <span class="math inline">\(\beta_{1}\)</span> = 0? Why? What does this mean in terms of the relationship between body and heart weight, in non-technical terms 2B) What proportion of the variance of heart weight is explained by body weight, the predictor?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">coef</span>(slr1) <span class="co"># Use help(coef) to understand what this function does</span></code></pre></div>
<pre><code>## (Intercept)       Hwt_g 
##   1.0196367   0.1602902</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">intcpt &lt;-<span class="st"> </span><span class="kw">coef</span>(slr1)[<span class="dv">1</span>] <span class="co"># extract estimated intercept, or &quot;a&quot; as it is called in the SG textbook</span>

slope &lt;-<span class="st"> </span><span class="kw">coef</span>(slr1)[<span class="dv">2</span>]<span class="co"># Code needed here. Extract the estimated slope or b, just like we did with the intercept. </span>
  
  
Hwt.hat &lt;-<span class="st"> </span>intcpt <span class="op">+</span><span class="st"> </span>slope <span class="op">*</span><span class="st"> </span>cats<span class="op">$</span>Bwt <span class="co"># creates a vector of estimated or predicted heart weights for each of the measured days using the regression model coefficients</span>


errors &lt;-<span class="st"> </span>cats<span class="op">$</span>Hwt <span class="op">-</span><span class="st"> </span>Hwt.hat <span class="co"># creates a vector of the differences between the sample heart weight measurements and the predicted weights from the model, i.e., these are the “residuals”</span>


(TotalSS &lt;-<span class="st"> </span><span class="kw">sum</span>((cats<span class="op">$</span>Hwt <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(cats<span class="op">$</span>Hwt)) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>)) <span class="co"># total sums of squares</span></code></pre></div>
<pre><code>## [1] 847.6256</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(SSErrors &lt;-<span class="st"> </span><span class="kw">sum</span>(errors <span class="op">^</span><span class="st"> </span><span class="dv">2</span>)) <span class="co"># Code needed here. Type in the vector of the residuals to calculate residual sum of squares</span></code></pre></div>
<pre><code>## [1] 12925.23</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(SSReg &lt;-<span class="st"> </span>TotalSS <span class="op">-</span><span class="st"> </span>SSErrors) <span class="co"># regression sums of squares</span></code></pre></div>
<pre><code>## [1] -12077.6</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n &lt;-<span class="st"> </span><span class="kw">length</span>(cats<span class="op">$</span>Hwt) <span class="co"># Code needed here. Complete the code with the column that contains the observations</span>

(dfe &lt;-<span class="st"> </span>n <span class="op">-</span><span class="st"> </span><span class="kw">length</span>(<span class="kw">coef</span>(slr1))) <span class="co"># df of the residuals</span></code></pre></div>
<pre><code>## [1] 142</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(dfTotal &lt;-<span class="st"> </span>n<span class="op">-</span><span class="dv">1</span>) <span class="co"># Complete the code to calculate the total df</span></code></pre></div>
<pre><code>## [1] 143</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(dfReg &lt;-<span class="st"> </span><span class="dv">1</span>) <span class="co"># Complete the code to calculate the df of the regression</span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(MSReg &lt;-<span class="st"> </span>SSReg <span class="op">/</span><span class="st"> </span>dfReg)</code></pre></div>
<pre><code>## [1] -12077.6</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(MSE &lt;-<span class="st"> </span>SSErrors <span class="op">/</span><span class="st"> </span>dfe) <span class="co"># Complete the code to calculate the MSE </span></code></pre></div>
<pre><code>## [1] 91.02274</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(Fcalc &lt;-<span class="st"> </span>MSReg <span class="op">/</span><span class="st"> </span>MSE) <span class="co"># Calculated F statistic from the data </span></code></pre></div>
<pre><code>## [1] -132.6878</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(Fcritical &lt;-<span class="st"> </span><span class="kw">qf</span>(<span class="fl">0.95</span>, dfReg, dfe)) <span class="co"># Complete the function arguments with the degrees of freedom for the numerator and the denominator to get the F critical value.</span></code></pre></div>
<pre><code>## [1] 3.907782</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(slr1) <span class="co"># check your previous “by hand” calculation against the anova table.</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: Bwt_Kg
##            Df Sum Sq Mean Sq F value    Pr(&gt;F)    
## Hwt_g       1 21.778 21.7780  259.83 &lt; 2.2e-16 ***
## Residuals 142 11.902  0.0838                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p><strong>ANSWER THE FOLLOWING QUESTIONS:</strong></p>
<p>2A) Do you reject the Ho: <span class="math inline">\(\beta_{1}\)</span> = 0? Why? What does this mean in terms of the relationship between body weight and heart weight, in non-technical terms</p>
<p>2B) What proportion of the variance of heart weight is explained by body weight, the predictor?</p>
</div>
<div id="part-3.-make-a-95-confidence-interval-for-the-estimation-of-the-slope.-25-points" class="section level3">
<h3><span class="header-section-number">1.13.9</span> Part 3. Make a 95% confidence interval for the estimation of the slope. [25 points]</h3>
<p>First, do the calculations by hand, using the formulas from the textbook 179-180. Then use R functions.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Hint: check your answers with the anova table. It should be the same if your calculations are correct</span>


(tcritical.slr1 &lt;-<span class="st"> </span><span class="kw">qt</span>(<span class="fl">0.975</span> , dfe)) <span class="co"># Code needed here.  Enter the probability for a 2-tailed test with alpha = 0.05</span></code></pre></div>
<pre><code>## [1] 1.976811</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">SSX &lt;-<span class="st"> </span><span class="kw">sum</span>((cats<span class="op">$</span>Bwt <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(cats<span class="op">$</span>Bwt)) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>) <span class="co"># get sum of square of X</span>


(se.beta &lt;-<span class="st"> </span><span class="kw">sqrt</span>(SSErrors <span class="op">/</span><span class="st"> </span>(dfe <span class="op">*</span><span class="st"> </span>SSX))) <span class="co"># calculate the s.e. for beta</span></code></pre></div>
<pre><code>## [1] 1.643958</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">beta.lo &lt;-<span class="st"> </span>slope <span class="op">-</span><span class="st"> </span>tcritical.slr1 <span class="op">*</span><span class="st"> </span>se.beta <span class="co"># lower CI boundary.  </span>


beta.hi &lt;-<span class="st"> </span>slope <span class="op">+</span><span class="st"> </span>tcritical.slr1 <span class="op">*</span><span class="st"> </span>se.beta <span class="co"># Code needed here. Complete the formula for the the upper CI boundary</span>


<span class="kw">c</span>(beta.lo, beta.hi) <span class="co"># Display the lower and upper CI extremes </span></code></pre></div>
<pre><code>##     Hwt_g     Hwt_g 
## -3.089504  3.410084</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(slr1)[<span class="dv">2</span>,] <span class="co"># easier way to get the CI using the function from R</span></code></pre></div>
<pre><code>##     2.5 %    97.5 % 
## 0.1406330 0.1799475</code></pre>
<p><strong>ANSWER THE FOLLOWING QUESTIONS:</strong></p>
<p>3A) What is the 95% confidence interval for the relationship between body weight and heart weight?</p>
</div>
<div id="part-4.-make-a-95-confidence-interval-for-mean-heart-weight-at-a-given-body-weight.-20-points" class="section level3">
<h3><span class="header-section-number">1.13.10</span> Part 4. Make a 95% confidence interval for mean heart weight at a given body weight. [20 points]</h3>
<p>Assume that you are interested in using the fitted regression model (the line) to estimate the mean heart weight of cats when they are 2.9 kg. Use the regression equation to make the estimate and then calculate the standard error of the estimate to make a confidence interval for it. Do detailed calculations first and then use R functions.</p>
<p>You will first calculate the predicted mean of Y for 2.9 kg. and its standard error and make a confidence interval for it. Then you will calculate the standard error for a predicted individual Y and compare it.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(Hwt.hat.<span class="fl">2.9</span> &lt;-<span class="st"> </span>intcpt <span class="op">+</span><span class="st"> </span>slope <span class="op">*</span><span class="st"> </span><span class="fl">2.9</span>) <span class="co"># In other words, Y = a + bX, where Y, or the predicted value, is the weight of plants at 33 days.  </span></code></pre></div>
<pre><code>## (Intercept) 
##    1.484478</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(g.hat.<span class="fl">2.9</span> &lt;-<span class="st"> </span><span class="kw">exp</span>(Hwt.hat.<span class="fl">2.9</span>))  <span class="co"># Since the response variable is log transformed, here we need to do a back transformation.</span></code></pre></div>
<pre><code>## (Intercept) 
##    4.412663</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Calculate the standard error of the predicted mean of Y:</span>
se.Hwt.<span class="fl">2.9</span>.m &lt;-<span class="st"> </span><span class="kw">sqrt</span>(MSE <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">/</span><span class="st"> </span>n <span class="op">+</span><span class="st"> </span>(<span class="fl">2.9</span> <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(cats<span class="op">$</span>Bwt_Kg)) <span class="op">^</span><span class="st"> </span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span>SSX)) <span class="co"># SG book pg. 192 and pg. d205 No. 6</span>


<span class="co"># Complete the code to calculate the upper and lower CI for the heart weight of cats when they are 2.9 kg:</span>
(CI.Hwt.up &lt;-<span class="st"> </span>Hwt.hat.<span class="fl">2.9</span> <span class="op">+</span><span class="st"> </span>tcritical.slr1 <span class="op">*</span><span class="st"> </span>se.Hwt.<span class="fl">2.9</span>.m)</code></pre></div>
<pre><code>## (Intercept) 
##    3.157412</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(CI.Hwt.lo &lt;-<span class="st"> </span>Hwt.hat.<span class="fl">2.9</span> <span class="op">-</span><span class="st"> </span>tcritical.slr1 <span class="op">*</span><span class="st"> </span>se.Hwt.<span class="fl">2.9</span>.m)</code></pre></div>
<pre><code>## (Intercept) 
##  -0.1884555</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pred.data &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">Hwt_g =</span> g.hat.<span class="fl">2.9</span>, <span class="dt">Bwt_kg =</span> <span class="fl">2.9</span>) <span class="co"># Here we create a data frame that we will use to “predict” or estimate heart weight in the next line.  Only the value for body weight is included since that is the X, independent, or predictor variable.</span>


(ci.r &lt;-<span class="st"> </span><span class="kw">predict</span>(slr1, <span class="dt">newdata =</span> pred.data, <span class="dt">interval =</span> <span class="st">&quot;confidence&quot;</span>)) <span class="co"># this line uses predict() to predict or estimate heart weight and its CI, as we did by hand above.</span></code></pre></div>
<pre><code>##                  fit      lwr      upr
## (Intercept) 1.726944 1.595742 1.858145</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># The interval = “confidence” argument makes the function return a confidence interval for the estimated log of plant mass.</span>

<span class="kw">exp</span>(ci.r) <span class="co"># Complete the code to do a back transformation on the predicted values.</span></code></pre></div>
<pre><code>##                 fit      lwr      upr
## (Intercept) 5.62344 4.931986 6.411835</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Calculate the standard error of the predicted individual Y:</span>
se.Hwt.<span class="fl">33.</span>ind &lt;-<span class="st"> </span><span class="kw">sqrt</span>(MSE <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span>n <span class="op">+</span><span class="st"> </span>(<span class="fl">2.9</span> <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(cats<span class="op">$</span>Bwt_Kg)) <span class="op">^</span><span class="st"> </span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span>SSX)) <span class="co"># SG book pg. 205 No. 7</span></code></pre></div>
<p><strong>ANSWER THE FOLLOWING QUESTIONS:</strong></p>
<p>4A) Explain why we need to use exp().</p>
<p>4B) Report the upper and lower extremes of the CI for the predicted mean of Y</p>
<p>4C) Is the standard error estimate for a predicted mean of Y or a predicted individual Y based on the model bigger? Why?</p>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>


    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
},
"search": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
